{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "import transformers\r\n",
    "from transformers import AutoModel, BertTokenizerFast\r\n",
    "\r\n",
    "device = torch.device(\"cuda\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv(\"pretest_for_model.csv\")\r\n",
    "df = df.drop(columns=['Processed'])\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            Original  PrivacyRatio\n",
       "0  patents, designs, algorithms, utility models, ...             4\n",
       "1  Continue shopping Go to cart OK Home > Terms &...             3\n",
       "2  The following Sections of this PSO Addendum wi...             3\n",
       "3  Lingo Bingo (“Prize Draw”) is organised by Hut...             0\n",
       "4  Chronus will consider all the available inform...             5"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>PrivacyRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patents, designs, algorithms, utility models, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Continue shopping Go to cart OK Home &gt; Terms &amp;...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The following Sections of this PSO Addendum wi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lingo Bingo (“Prize Draw”) is organised by Hut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chronus will consider all the available inform...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for i, row in df.iterrows():\r\n",
    "    if row['PrivacyRatio'] >= 3:\r\n",
    "        df.loc[i, 'PrivacyRatio'] = 1\r\n",
    "    else:\r\n",
    "        df.loc[i, 'PrivacyRatio'] = 0\r\n",
    "df.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            Original  PrivacyRatio\n",
       "0  patents, designs, algorithms, utility models, ...             1\n",
       "1  Continue shopping Go to cart OK Home > Terms &...             1\n",
       "2  The following Sections of this PSO Addendum wi...             1\n",
       "3  Lingo Bingo (“Prize Draw”) is organised by Hut...             0\n",
       "4  Chronus will consider all the available inform...             1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>PrivacyRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patents, designs, algorithms, utility models, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Continue shopping Go to cart OK Home &gt; Terms &amp;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The following Sections of this PSO Addendum wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lingo Bingo (“Prize Draw”) is organised by Hut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chronus will consider all the available inform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# split train dataset into train, validation and test sets\r\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['Original'], df['PrivacyRatio'], random_state=2021, test_size=0.3)\r\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, random_state=2021, test_size=0.5)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# import BERT-base pretrained model\r\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "# Load the BERT tokenizer\r\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# tokenize and encode sequences in the training set\r\n",
    "tokens_train = tokenizer.batch_encode_plus(\r\n",
    "    train_text.tolist(),\r\n",
    "    max_length = 459,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True\r\n",
    ")\r\n",
    "\r\n",
    "# tokenize and encode sequences in the validation set\r\n",
    "tokens_val = tokenizer.batch_encode_plus(\r\n",
    "    val_text.tolist(),\r\n",
    "    max_length = 459,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True\r\n",
    ")\r\n",
    "\r\n",
    "# tokenize and encode sequences in the test set\r\n",
    "tokens_test = tokenizer.batch_encode_plus(\r\n",
    "    test_text.tolist(),\r\n",
    "    max_length = 459,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\USER-PC\\anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2126: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## convert lists to tensors\r\n",
    "\r\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\r\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\r\n",
    "train_y = torch.tensor(train_labels.tolist())\r\n",
    "\r\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\r\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\r\n",
    "val_y = torch.tensor(val_labels.tolist())\r\n",
    "\r\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\r\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\r\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
    "\r\n",
    "#define a batch size\r\n",
    "batch_size = 32\r\n",
    "\r\n",
    "# wrap tensors\r\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\r\n",
    "\r\n",
    "# sampler for sampling the data during training\r\n",
    "train_sampler = RandomSampler(train_data)\r\n",
    "\r\n",
    "# dataLoader for train set\r\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
    "\r\n",
    "# wrap tensors\r\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\r\n",
    "\r\n",
    "# sampler for sampling the data during training\r\n",
    "val_sampler = SequentialSampler(val_data)\r\n",
    "\r\n",
    "# dataLoader for validation set\r\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# freeze all the parameters\r\n",
    "for param in bert.parameters():\r\n",
    "    param.requires_grad = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class BERT_Arch(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, bert):\r\n",
    "      \r\n",
    "      super(BERT_Arch, self).__init__()\r\n",
    "\r\n",
    "      self.bert = bert \r\n",
    "      \r\n",
    "      # dropout layer\r\n",
    "      self.dropout = nn.Dropout(0.1)\r\n",
    "      \r\n",
    "      # relu activation function\r\n",
    "      self.relu =  nn.ReLU()\r\n",
    "\r\n",
    "      # dense layer 1\r\n",
    "      self.fc1 = nn.Linear(768,512)\r\n",
    "      \r\n",
    "      # dense layer 2 (Output layer)\r\n",
    "      self.fc2 = nn.Linear(512,2)\r\n",
    "\r\n",
    "      #softmax activation function\r\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\r\n",
    "\r\n",
    "    #define the forward pass\r\n",
    "    def forward(self, sent_id, mask):\r\n",
    "\r\n",
    "      #pass the inputs to the model  \r\n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\r\n",
    "      \r\n",
    "      x = self.fc1(cls_hs)\r\n",
    "\r\n",
    "      x = self.relu(x)\r\n",
    "\r\n",
    "      x = self.dropout(x)\r\n",
    "\r\n",
    "      # output layer\r\n",
    "      x = self.fc2(x)\r\n",
    "      \r\n",
    "      # apply softmax activation\r\n",
    "      x = self.softmax(x)\r\n",
    "\r\n",
    "      return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# pass the pre-trained BERT to our define architecture\r\n",
    "model = BERT_Arch(bert)\r\n",
    "\r\n",
    "# push the model to GPU\r\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# optimizer from hugging face transformers\r\n",
    "from transformers import AdamW\r\n",
    "\r\n",
    "# define the optimizer\r\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)          # learning rate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\r\n",
    "\r\n",
    "#compute the class weights\r\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\r\n",
    "\r\n",
    "print(\"Class Weights:\",class_weights)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class Weights: [2.13414634 0.65298507]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\USER-PC\\anaconda3\\envs\\bert\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=48     1\n",
      "103    0\n",
      "113    1\n",
      "337    1\n",
      "269    1\n",
      "      ..\n",
      "109    0\n",
      "128    1\n",
      "57     1\n",
      "341    1\n",
      "116    1\n",
      "Name: PrivacyRatio, Length: 350, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# converting list of class weights to a tensor\r\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\r\n",
    "\r\n",
    "# push to GPU\r\n",
    "weights = weights.to(device)\r\n",
    "\r\n",
    "# define the loss function\r\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \r\n",
    "\r\n",
    "# number of training epochs\r\n",
    "epochs = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# function to train the model\r\n",
    "def train():\r\n",
    "  \r\n",
    "  model.train()\r\n",
    "\r\n",
    "  total_loss, total_accuracy = 0, 0\r\n",
    "  \r\n",
    "  # empty list to save model predictions\r\n",
    "  total_preds=[]\r\n",
    "  \r\n",
    "  # iterate over batches\r\n",
    "  for step,batch in enumerate(train_dataloader):\r\n",
    "    \r\n",
    "    # progress update after every 50 batches.\r\n",
    "    if step % 50 == 0 and not step == 0:\r\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n",
    "\r\n",
    "    # push the batch to gpu\r\n",
    "    batch = [r.to(device) for r in batch]\r\n",
    " \r\n",
    "    sent_id, mask, labels = batch\r\n",
    "\r\n",
    "    # clear previously calculated gradients \r\n",
    "    model.zero_grad()        \r\n",
    "\r\n",
    "    # get model predictions for the current batch\r\n",
    "    preds = model(sent_id, mask)\r\n",
    "\r\n",
    "    # compute the loss between actual and predicted values\r\n",
    "    loss = cross_entropy(preds, labels)\r\n",
    "\r\n",
    "    # add on to the total loss\r\n",
    "    total_loss = total_loss + loss.item()\r\n",
    "\r\n",
    "    # backward pass to calculate the gradients\r\n",
    "    loss.backward()\r\n",
    "\r\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
    "\r\n",
    "    # update parameters\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    # model predictions are stored on GPU. So, push it to CPU\r\n",
    "    preds=preds.detach().cpu().numpy()\r\n",
    "\r\n",
    "    # append the model predictions\r\n",
    "    total_preds.append(preds)\r\n",
    "\r\n",
    "  # compute the training loss of the epoch\r\n",
    "  avg_loss = total_loss / len(train_dataloader)\r\n",
    "  \r\n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
    "\r\n",
    "  #returns the loss and predictions\r\n",
    "  return avg_loss, total_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# function for evaluating the model\r\n",
    "def evaluate():\r\n",
    "  \r\n",
    "  print(\"\\nEvaluating...\")\r\n",
    "  \r\n",
    "  # deactivate dropout layers\r\n",
    "  model.eval()\r\n",
    "\r\n",
    "  total_loss, total_accuracy = 0, 0\r\n",
    "  \r\n",
    "  # empty list to save the model predictions\r\n",
    "  total_preds = []\r\n",
    "\r\n",
    "  # iterate over batches\r\n",
    "  for step,batch in enumerate(val_dataloader):\r\n",
    "    \r\n",
    "    # Progress update every 50 batches.\r\n",
    "    if step % 50 == 0 and not step == 0:\r\n",
    "      \r\n",
    "      # Calculate elapsed time in minutes.\r\n",
    "      elapsed = format_time(time.time() - t0)\r\n",
    "            \r\n",
    "      # Report progress.\r\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n",
    "\r\n",
    "    # push the batch to gpu\r\n",
    "    batch = [t.to(device) for t in batch]\r\n",
    "\r\n",
    "    sent_id, mask, labels = batch\r\n",
    "\r\n",
    "    # deactivate autograd\r\n",
    "    with torch.no_grad():\r\n",
    "      \r\n",
    "      # model predictions\r\n",
    "      preds = model(sent_id, mask)\r\n",
    "\r\n",
    "      # compute the validation loss between actual and predicted values\r\n",
    "      loss = cross_entropy(preds,labels)\r\n",
    "\r\n",
    "      total_loss = total_loss + loss.item()\r\n",
    "\r\n",
    "      preds = preds.detach().cpu().numpy()\r\n",
    "\r\n",
    "      total_preds.append(preds)\r\n",
    "\r\n",
    "  # compute the validation loss of the epoch\r\n",
    "  avg_loss = total_loss / len(val_dataloader) \r\n",
    "\r\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
    "\r\n",
    "  return avg_loss, total_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# set initial loss to infinite\r\n",
    "best_valid_loss = float('inf')\r\n",
    "\r\n",
    "# empty lists to store training and validation loss of each epoch\r\n",
    "train_losses=[]\r\n",
    "valid_losses=[]\r\n",
    "\r\n",
    "#for each epoch\r\n",
    "for epoch in range(epochs):\r\n",
    "     \r\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n",
    "    \r\n",
    "    #train model\r\n",
    "    train_loss, _ = train()\r\n",
    "    \r\n",
    "    #evaluate model\r\n",
    "    valid_loss, _ = evaluate()\r\n",
    "    \r\n",
    "    #save the best model\r\n",
    "    if valid_loss < best_valid_loss:\r\n",
    "        best_valid_loss = valid_loss\r\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
    "    \r\n",
    "    # append training and validation loss\r\n",
    "    train_losses.append(train_loss)\r\n",
    "    valid_losses.append(valid_loss)\r\n",
    "    \r\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.701\n",
      "Validation Loss: 0.700\n",
      "\n",
      " Epoch 2 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.695\n",
      "Validation Loss: 0.697\n",
      "\n",
      " Epoch 3 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.690\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 4 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.697\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 5 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.695\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 6 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.693\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 7 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.693\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 8 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.689\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 9 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.695\n",
      "Validation Loss: 0.696\n",
      "\n",
      " Epoch 10 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.690\n",
      "Validation Loss: 0.696\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#load weights of best model\r\n",
    "path = 'saved_weights.pt'\r\n",
    "model.load_state_dict(torch.load(path))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# get predictions for test data\r\n",
    "with torch.no_grad():\r\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\r\n",
    "  preds = preds.detach().cpu().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "preds = np.argmax(preds, axis = 1)\r\n",
    "print(classification_report(test_y, preds))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.20      0.19        15\n",
      "           1       0.79      0.77      0.78        60\n",
      "\n",
      "    accuracy                           0.65        75\n",
      "   macro avg       0.48      0.48      0.48        75\n",
      "weighted avg       0.67      0.65      0.66        75\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('bert': conda)"
  },
  "interpreter": {
   "hash": "40b9cf565d070ee13d37836166faa1e65075ac740510169fa1bf4f89f6b88bdd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}